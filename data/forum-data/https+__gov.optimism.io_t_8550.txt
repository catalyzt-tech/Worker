{
  "title": "Joan's RPGF4 Reflections",
  "content": "In this post I will try to share some points that I take away from the process\nof participating as a reviewer and appeals reviewer in RPGF4.\n\nIn a second post (below) I will share my reflections on the voting process.\n\n\nPRECONDITIONS\n\nRound scope\n\nThe aim of RPGF4\n[https://gov.optimism.io/t/retro-funding-4-onchain-builders-round-details/7988]\nis to reward onchain builders who have deployed contracts to the Superchain and\ncontributed to the success of Optimism by increasing demand for blockspace and\ndriving value to the Collective.\n\nMetrics-based evaluation\n\nAlong with a more narrow focus, this round also differed from the previous round\n[https://gov.optimism.io/t/joans-rpgf3-reflections/7221] in that the voting\ndesign prescribed a new form of automated, metrics-based evaluation.\n\nDetails on the thinking behind the metrics-based design can be found here\n[https://gov.optimism.io/t/experimentation-impact-metric-based-voting/7727], and\nI would like to repeat\n[https://gov.optimism.io/t/experimentation-impact-metric-based-voting/7727/9]\nwhat is pointed out in that document:\n\n> A metrics based voting experience is only viable for a subset of impact today\n> and can’t be applied to evaluate all contributions to the Optimism Collective.\n\nAt the same time, an objective and metrics-based approach - for better and for\nworse - rules out subjective human judgement at times when it might be tempting\nto use it. There have definitely been times during this round when I felt\nfrustrated about this.\n\n\n\n[https://europe1.discourse-cdn.com/bc41dd/optimized/2X/1/1bc528d84302e615697feb33320d30db213c3aa5_2_345x173.png]\n1128×566 88.7 KB\n[https://europe1.discourse-cdn.com/bc41dd/original/2X/1/1bc528d84302e615697feb33320d30db213c3aa5.png]\n\n\n\nWith that said, I appreciate the idea of smaller, more targeted rounds and the\npossibility to experiment with different ways of evaluating impact.\n\nImpact definition\n\nI’m glad that the decision was made to use the Foundation’s fallback definition\nof “profit = impact”, implying that no previous grants would be subtracted from\nthis round’s OP allocation.\n\nWhile I do think that previous funding should ideally be taken into account, I\nbelieve that the road towards formally defining ‘impact’ and ‘profit’ is still\nlong\n[https://gov.optimism.io/t/ratification-of-profit-definition-for-round-4/8312/29],\nand we should not rush it or pretend that we are already there.\n\nExperimental attitude\n\nAs mentioned, I value the experimental attitude behind Optimism’s RPGF.\n\nPart of this, it seems to me, is to accept the possibility of failure.\n\nWhile this round does have some features that I find problematic, it is\nessential in any experiment to allow things to run their course and not\naccidentally hide possible design flaws by trying to apply common sense on an\nindividual basis.\n\n\nREVIEW PROCESS\n\nI spent quite a lot of hours on the review process, as a reviewer and appeals\nreviewer.\n\nTwo main things, I think, could relatively easily be improved in future rounds:\nMore clarity around what reviewers need or need not check, and the software\ntools available to reviewers.\n\nWhat must reviewers check?\n\nAs reviewers, we were offered a handy reviewer guide\n[https://app.charmverse.io/op-retrofunding-review-process/optimism-retrofunding-round-4-application-review-19377272787222233].\nBut while this document contained just a short list of fairly specific things to\nlook for (“What to review projects on”), reviewers are also expected to ensure\nthat applications comply with the general application rules, and it is my\nimpression that this left quite a bit of room for subjective interpretation -\nand uncertainty.\n\nSome specific points around which there seemed to be ambiguity:\n\n * Github repo age. The guidelines say: “Does the repo hold no code or is it\n   newly created, raising a red flag?” What is a red flag? A warning signal? A\n   disqualification?\n\n * Unverified contracts. Reviewers are being asked to verify that the applicant\n   has “made their contract code available in a public Github repo”. What\n   happens when a contract shows as unverified on e.g. optimistic etherscan? How\n   would a reviewer know if the deployed contract corresponds to the code in the\n   Github repo?\n\n * OSO slug. This was arguably the most mysterious\n   [https://discord.com/channels/667044843901681675/1163942190104461444/1251355792846880949]\n   topic in the review process.\n\n * Downstream contracts. During the review process, it surfaced that the\n   automatic verification performed by OSO considered anything downstream\n   [https://discord.com/channels/667044843901681675/1163942190104461444/1255124780906971228]\n   of a project’s stated deployer as in scope. While it makes sense that it\n   could be impractical for some projects to explicitly list all of their\n   contracts in their application, this does raise a number of questions\n   [https://discord.com/channels/667044843901681675/1163942190104461444/1255127547335807027]\n   with regard to the review process and the task of the reviewers. In\n   particular, exactly what code do reviewers need to attest to having seen?\n\n\n\n[https://europe1.discourse-cdn.com/bc41dd/optimized/2X/d/d4a011768b7e4c79def4d05606b211dc50c28667_2_517x371.png]\n1620×1164 204 KB\n[https://europe1.discourse-cdn.com/bc41dd/original/2X/d/d4a011768b7e4c79def4d05606b211dc50c28667.png]\n\n\n\nIdeally, reviewers should be given instructions that clearly specify what action\nthey must take when they see this or that. And they need to know exactly what\nthey need to look for.\n\nThis requires the reviewers to understand the automatic verification process\nclearly, such that they know what they can safely ignore.\n\nEspecially in a round like this with so much emphasis on objectivity and\nquantitative metrics, it seems important that eg. overlapping impact,\ntransaction counts and transaction timing should be automatically checked …and\npossibly subjected to random sample tests by appointed badgeholders with the\nnecessary skillset.\n\nOther badgeholders could be asked to notify these specialists in case of doubt,\nbut abstain from rejecting applications based on their own tests.\n\nI think more clarity will lead to more consistency and delegation of\nresponsibility. Feedback and questions from the human reviewers can be used to\ninform the automatic process - and the automatic process can apply the learnings\nconsistently.\n\nReviewer tooling\n\nSuggestions for improvement of the Charmverse UI that was used in the review\nprocess:\n\n * Filtering. Reviewers absolutely need to be able to easily see which projects\n   are assigned to them and the current status of these projects. The same is\n   true for appeals reviewers.\n\n * Notifications. It would be awesome if there was some kind of automatic\n   notification in case a reviewer has missed a review, and also whenever new\n   projects are assigned to a person.\n\n * Rejection reasons. We need the option to reject an application due to missing\n   or incomplete information. It does not feel good to choose ‘spam’ or\n   ‘deceiving badgeholders’ in cases where an application is simply incomplete\n   for reasons that the reviewer can’t know.\n\n * Reviewer notes. The current interface offers 3 kinds of free text comments:\n   a) The almost invisible ‘reviewer notes’ in the top right corner; b) the\n   public comments on the bottom of the screen; and c) the explanation attatched\n   to rejections. I think the reviewer notes are of little use at this point,\n   and it might be better to remove them completely to avoid confusion. It would\n   be nice to have the option of also offering an explanation when you accept an\n   application. The public notes on the bottom of the page actually proved quite\n   useful in one or two cases where an applicant pitched in and explained\n   something.\n\n * Application links. Links to project website, Github, contracts etc. really\n   should be shown as links and not just as text. Having to copy-paste these\n   things into new browser tabs every time very, very quickly grows tiresome!\n\n * Changing votes. The flow around a reviewer changing his opinion could be\n   better. In some cases, trying to change a vote also resulted in the\n   cancellation of the four other reviewers’ votes - which was all the more\n   problematic as due to anonymized user names it wasn’t always possible to see\n   who you needed to contact if you had accidentally done this…\n\n * Data integrity. It is really important that reviewers can see exactly what\n   applicants put in their applications. In a few cases, bugs in the UI\n   prevented reviewers from accurately seeing parts of the application data.\n   Applications were mistakenly being rejected because of this until the bugs\n   were discovered.\n\nIn a future round, maybe a few reviewers could be employed to test out the\nsoftware before the start of the actual review process?\n\nIt would also be good to allow/request applicants to review their own\napplications in the same UI(s) that reviewers and voters will be using - that\nway, any data integrity issues are much more likely to be discovered early on.\n\nAlso, maybe some sort of collaboration could happen between Charmverse and\nRetrolist.app. In this round, I would say that Retrolist did a better job at\nreliably mirroring the application data and providing helpful links etc. But\nreviewers still had to use Charmverse because that’s where the voting\nfunctionality was.",
  "views": 312,
  "like_count": 19,
  "word_count": 3252,
  "answer": [
    {
      "content": "VOTING PROCESS\n\n\nVOTING FORMAT\n\nIn RPGF4, badgeholders were asked to vote on metrics, not applications or\nprojects.\n\n16 possible metrics were available, and each badgeholder was allowed to mix and\nmatch these as he liked and give each chosen metric a weight (a percentage, such\nthat all chosen metrics would add up to 100%).\n\nIn addition, there was the option of applying an ‘open source reward multiplier’\nwhich would multiply the effects of applying the chosen metrics across open\nsource projects by up to 3x.\n\n\nTIME SPENT\n\nCompared to previous rounds, the voting process was extremely ‘efficient’: It\ndoes not take long to add a few metrics to a ballot and submit them.\n\nIncluding the pre-voting workshop on metrics, a kickoff call, time to read up on\neach metric and reflecting on their usefulness, some experimenting in the actual\nvoting interface, as well as participating in a bit of discussion on telegram\nand a few badgeholder calls to exchange notes and ideas - I probably spent less\nthan 10 hours in total on the voting part of RPGF4. For comparison, I spent more\ntime on the review process.\n\n(Writing this takes time too, of course, but hey -)\n\n\nDISCUSSIONS\n\nAside from a couple of dedicated discussion calls with a handful of\nparticipants, it is my impression that there was less discussion in the\nbadgeholder group in this round than in the previous round.\n\n\nVALUES AND ETHICS\n\nMy main critique in this round is that the format rules out ‘subjective’\nevaluations - ie. ethics.\n\nRewarding any and all kinds of onchain activity feels wrong to me. What kind of\nPhoenix [https://medium.com/ethereum-optimism/ethers-phoenix-18fb7d7304bb] will\nwe be summoning like that?\n\nDebating what actually constitutes a ‘public good’ takes time and energy, and we\nwill likely never agree\n[https://discord.com/channels/667044843901681675/1163942190104461444/1250121415630393415]\n- but the same is true of ‘good objective metrics’. We need the discussions, and\nwe need to find ways to make decisions that don’t just ignore the complex human\nreality.\n\nGenerating gas fees, or initiating transactions, or even having many human\nusers, does not necessarily mean that your impact is good.\n\nAnd even though we may not be able to collectively define a ‘public good’ in the\nsame concise way that we define ‘gas fee’, we can certainly create mechanisms\nthat allow human citizens to weigh in on what is or isn’t a public good - in the\nsame way that we, in this round, were allowed to weigh in on what is or isn’t a\ngood metric.\n\n\nPUBLIC VOTING\n\nVotes will be public this time. I don’t feel good about that.\n\nIt is true that choosing metrics rather than applications makes things less\n‘personal’. And the risk of voters selling votes or voting under threat would be\nthere in any case, as the online voting happens unsupervised and anyone can take\na photo of their own ballot and share it (which means that they can also be\npressured into doing so).\n\nStill, with a ‘secret’ vote - possibly subject to checks performed by appointed\nscrutineers working in confidentiality - badgeholders would have a fighting\nchance of remaining relatively anonymous and just doing their work. Or even be\npublic figures who keep their actual voting to themselves.\n\nDifferent badgeholders may approach these things differently, or even adjust\ntheir approach over time according to their life situation.\n\nWith a public vote, all badgeholders are being pushed out into the public\nspotlight and placed in a position where they could have to defend their votes.\nThat may be appropriate for delegates (who are representatives), but imo not for\ncitizens.\n\nRPGF involves allocating large sums of money, and debates on Twitter and Discord\ndo get very heated at times. Badgeholders are humans, and I’m seeing\nsimultaneous moves from various sides towards wanting to know more about us,\nanalyze our onchain activity, require us to use a Farcaster account with\nverified addresses, pull in our ENS, etc. In the near future, whether we like it\nor not, many of us will be very easy to track down by a disgruntled applicant,\nonline and offline.\n\nIn the past months, between RPGF3 and RPGF4, I have received messages on various\nplatforms from people who reached out to me only because I’m an Optimism\nbadgeholder. That is what it is - I don’t necessarily see anything wrong with\nreaching out. I have also received an offer of free tickets for a real life\nevent. To be honest, I have no idea when a friendly offer like that crosses the\nline to something that might be understood as a bribe. My educational background\noffers no help with this.\n\nIn any case, there is certainly a gray area. Some people will go to some lengths\nto try and ensure support for their projects in the future. I do my best to\navoid conflicts of interest. Knowing that my vote in RPGF3 was secret/anonymous\nhas helped me stay neutral at times when I might have otherwise felt under\npressure. With public voting an important layer of protection falls away.\nPsychologically, and in a worst-case scenario also physically.\n\n\nMETRICS UNDER CONSIDERATION\n\nIn line with my view on public voting I will not share my ballot here. If anyone\nreally badly wants to know, they can look it up later when all of the votes are\ndisclosed.\n\nBut I’m happy to share my reflections on the metrics. (And I have obviously\nvoted according to the views that I present here).\n\nGas fees and transactions\n\nGas fees will feed future retro funding rounds. As such it is a very clean and\nobvious metric to use. Even if a contract does nothing meaningful or good aside\nfrom that, by generating gas fees it does help keep Optimism alive. (I hope we\nall want Optimism to do more than just grow fat, but it certainly needs to feed\nitself).\n\nTransactions is a simpler metric than gas fees. Transactions must take place for\ngas fees to be generated, but gas fees also depend on the size and timing and\nperceived value of transactions. Many transactions could mean a lot of\nmeaningful activity - or spam, or clumsy coding. I don’t think this metric\ncaptures anything important that is not already captured, better, by ‘gas fees’.\n\nAddresses vs. users\n\nI’m torn here.\n\nOn the one hand, I would like to see Optimism as a great habitat for humans. Not\njust bots. Many human users could mean that a project has the support of -\nand/or supports - a lot of humans. That would be a potential indicator of a\npublic good.\n\nOn the other hand, machines can do great work too. They can certainly contribute\nto public goods, and in many contexts we absolutely need them. Using machines\nfor good should be rewarded.\n\nEven more importantly: Humans, too, can be used in many ways. Depending on the\ncontext, a user can be a customer, an employee, a slave or…?\n\nAddictive, bribing and attention-grabbing platforms may have many users.\n\nIn the physical world, you would (I hope) never blindly reward someone for\nhaving many employees without asking about the company’s product and the working\nconditions.\n\nI’m very wary about rewarding the use of humans without some kind of holistic\nhuman evaluation of the process.\n\nTrusted users, power users and badgeholders\n\nWe can think of different ways to recognize a human user, and we can even go a\nstep further and discuss what particular ‘types’ of users indicate about a\nproject.\n\nThis could be very useful - but only when we have some context.\n\nApplications operated by machines obviously don’t have trusted users.\n\nUsers who like their privacy will not be trusted. Applications that offer\nprivacy will not have a lot of trusted users.\n\nFurthermore, by making OP rewards dependent on specific groups of users, we\nmight end up incentivizing platforms to optimize around the use of those\nspecific humans. Being offered free tickets may still be good fun, but do we\nreally want to encourage all of the platforms out there to be constantly\ngrabbing for the attention and action of a few known badgeholders and their\nfriends - by making their funding dependent on it? I don’t.\n\nDaily vs. monthly\n\nI see no reason why a good contract would necessarily require constant activity.\n\nA combined requirement for ‘daily’ and ‘human’ could potentially create some\nvery unhealthy incentives. (Of course, some great projects might also score high\non such a metric. But the metric in itself does not tell us if a project is\ngreat or dystopian.)\n\nLinear vs. logscale\n\nA logscale has the very nice property that rewards are distributed according to\nmerit; projects that score higher on some metric M will get more than those that\nscore less, but rewards are not only funnelled towards a few giants that dwarf\neveryone else. Instead, the funds are distributed in a way that incentivizes\ninitiative of all sizes, promoting diversity and competition.\n\nLet’s be clear: Impact = profit can not mean that any project which generates $x\nin gas fees should receive $x in retro funding. That would be both silly,\ndangerous and very bad business.\n\nOpen source multiplier\n\nThere has been a lot of discussion around this metric. I recognize that it may\nnot entirely work as intended yet. But that is true for RPGF in general.\n\nI would like to point out that ‘open source’ is defined as a collective value\n[https://gov.optimism.io/t/collective-values/6985] of Optimism. One of the\nvalues that badgeholders are currently being asked to promise to uphold.\n\nGiven this, we should take our own medicine and create an incentive for us all\nto help us get better at defining and recognizing open source. So - I endorse\nthe use of this metric. That should make it worthwhile to put some effort into\nimproving the details.\n\n\nGOING FORWARD\n\nNow that we have tried this once, maybe we could experiment with more complex\nmetrics. Maybe something like “many human users is great, but only if\n[something]”?\n\nI could also imagine a setup where some kind of ‘subjective’ human filter is\napplied first (similar to what we saw in RPGF3, but maybe in more of a binary\nway similar to the initial review, to collectively decide which projects are\nindeed public goods) and then we apply objective metrics to decide who gets how\nmuch.\n\nWhat I would really love, though, is to experiment with using objective metrics\nto inform the decisions of humans, without mistaking the metric for the goal.",
      "created_at": "2024-07-12T16:17:28.766Z",
      "trust_level": 2,
      "username": "joanbp",
      "admin": false,
      "moderator": false,
      "staff": false,
      "like_count": 7
    },
    {
      "content": "Thank you for taking the time to provide such thoughtful and actionable\nfeedback. We’ll be sure to work with @Jonas [/u/jonas] and the team to improve\nthe experience for the next round.",
      "created_at": "2024-07-12T22:49:01.039Z",
      "trust_level": 2,
      "username": "alexpoon_hk",
      "admin": false,
      "moderator": false,
      "staff": false,
      "like_count": 3
    },
    {
      "content": "Awesome!\n\nThank you for your care and effort. :slightly_smiling_face:\n[https://emoji.discourse-cdn.com/twitter/slightly_smiling_face.png?v=12]",
      "created_at": "2024-07-15T08:21:46.688Z",
      "trust_level": 2,
      "username": "joanbp",
      "admin": false,
      "moderator": false,
      "staff": false,
      "like_count": 1
    }
  ],
  "created_at": "2024-07-08T18:13:31.285Z"
}