{
  "title": "OP Labs Audit Framework: When to get external security review and how to prepare for it",
  "content": "Hi all, maurelian from OP Labs here. In this post, we are sharing an internally\ndeveloped framework for identifying code which we believe should be audited\nprior to production deployment.\n\nOur purposes for publishing this framework are twofold:\n\n 1. Providing an example which other contributors to the OP Stack may take into\n    account when deciding how to get audits.\n 2. Seeking feedback from the community. Since Optimism Governance is the\n    ultimate decider of what gets shipped to production, it’s important to\n    ensure that the efforts made by core contributors like ourselves align with\n    community expectations.\n\n\nTHE FRAMEWORK\n\nThis framework provides guidance to teams at OP Labs who are writing security\ncritical code for the OP Stack and are interested in obtaining external review,\nincluding:\n\n * A security review or audit from a security firm.\n * A community audit competition from a platform such as Sherlock.\n * A bug bounty\n\nIt will describe what code should, from our perspective, be audited, and how to\nprepare for an audit.\n\n\nPHILOSOPHY BEHIND EXTERNAL SECURITY AUDITS\n\nIt is important to underscore that security starts with the developers. An audit\nshould not be seen as a means of “purchasing a security guarantee” from an\nexternal vendor.\n\nWhat a security vendor provides is:\n\n 1. Feedback and insight into your ability to write secure code.\n 2. A final assurance about the specific code you have developed.\n 3. A public artifact attesting to the security of the system.\n\n\nWHAT CODE SHOULD BE AUDITED?\n\nAudits are expensive and time consuming, so we recommend using them in a\ntargeted manner.\n\nGenerally, audits are better suited for some kinds of codebases, specifically\nthose where Safety is of Existential importance.\n\nThe following rubric divides projects along Liveness vs. Safety and Reputational\nvs. Existential axes.\n\n\n\naudit-rubric\n[https://europe1.discourse-cdn.com/bc41dd/optimized/2X/8/8d9928784ad86c4080e3d97a8d0c80f1b55970fb_2_588x499.png]\naudit-rubric2282×1940 303 KB\n[https://europe1.discourse-cdn.com/bc41dd/original/2X/8/8d9928784ad86c4080e3d97a8d0c80f1b55970fb.png]\n\n\n\nThe rubric above is not exhaustive, but should give a sense of the framework OP\nLabs uses to determine what requires an audit.\n\nMost OP Stack code fits into one of the following buckets:\n\n 1. Secure via testing and real world usage:\n    * This applies largely to infrastructural code, referring to components such\n      as op-node, op-geth, op-batcher as well as alternative implementations of\n      such components.\n    * In general, audits are less valuable or necessary for infrastructural code\n      because:\n      * Experience has shown us that auditors provide less value with\n        infrastructural code, especially relative to what is learned by getting\n        code running on a testnet or putting code in users’ hands.\n      * When issues do occur in infrastructural code, they can be addressed\n        through ‘social consensus’, ie. quickly disseminating new software with\n        a bug fix.\n 2. Secure with extreme caution (including auditing):\n    * This applies largely to smart contract code, in particular in cases where\n      assets are being secured.\n    * Examples include modifications to bridge code, changes to deposit\n      derivation code, multi-sig wallet modifications, etc.\n    * In general, audits are necessary for smart contract code because damage\n      resulting for vulnerabilities is more likely to be irrecoverable.\n\nThere are likely to be situations where smart contract code does not require an\naudit, and where infrastructure code should receive an audit. Every case\nrequires a fact-specific analysis about whether an audit should be required.\n\nAnd while there is no “magic formula” that works in all cases, analysis\naccording to the Existential vs. Reputational x Liveness vs. Safety framework is\nmore important than simply differentiating between infrastructure and smart\ncontract code.\n\n\nPREPARATION FOR EXTERNAL SECURITY REVIEW\n\nOnce again, secure code starts with the development teams that write it. We\nrecommend completing the following steps for a given codebase prior to seeking\nexternal review.\n\n 1. Enumerate the invariants and security properties of the system\n    \n    This step can be completed however the development team wishes, though teams\n    should note that it is an important input to the next two items.\n\n 2. Documentation for security researchers\n    \n    Security specific docs which describe the high-level security properties of\n    the system. A good example of this documentation is what OP Labs provided to\n    the Sherlock auditors [https://audits.sherlock.xyz/contests/63] for the\n    Bedrock audit.\n    Documentation for security researchers should outline:\n    \n    * The behavior that the system is expected to maintain.\n    * The negative situations the system should prevent.\n    * Known issues which are considered low risk and are out of scope.\n\n 3. Test coverage assessment\n    \n    Developers should evaluate coverage of the system from two angles:\n    \n    1. Code coverage\n       For each testing method, a test coverage report should be generated. This\n       report should be reviewed to identify any gaps in coverage.\n       We should aim for greater than 100% coverage of critical code paths. We\n       do not need to be sticklers about testing trivial code paths like basic\n       getters and such.\n    \n    2. Property coverage\n       \n       For each security property, a qualitative assessment of the test coverage\n       should be done. It should answer the following questions:\n       \n       * Has this property been thoroughly tested?\n       * Which testing methods have been used to test this property? Are there\n         other testing methods we could apply with a reasonable amount of\n         effort?\n       * Are there any edge cases relevant to this property which are not fully\n         tested?\n    \n    Page 13 and onwards of this Trail of Bits report\n    [https://github.com/ethereum-optimism/optimism/blob/develop/docs/security-reviews/2022_11-Invariant_Testing-TrailOfBits.pdf]\n    may serve as a useful template for this assessment.\n\n\nCONCLUSION\n\nWe believe that this framework provides a strong foundation for both development\nteams and governance in evaluating the necessity of an external review OP Stack\ncode.",
  "views": 1090,
  "like_count": 20,
  "word_count": 998,
  "answer": [
    {
      "content": "Many of us delegates are non-technical, and even the technical ones don’t have\nthe expertise or time to examine a complex codebase like OP Stack. So, a\ndiversity of external audits and testing reviews are key to our decisions. In\nfuture, when there’s a protocol upgrade proposal, I’d like to see a suite of\nreviews before the proposal is votable.",
      "created_at": "2023-09-26T23:26:37.007Z",
      "trust_level": 2,
      "username": "polynya",
      "admin": false,
      "moderator": false,
      "staff": false,
      "like_count": 6
    },
    {
      "content": "This is very helpful. I like the 2 OP stack buckets, but what do you prioritize\nif you have a time or a resource constraint? Or if you have any other priority\nsystem.",
      "created_at": "2023-09-26T12:15:44.659Z",
      "trust_level": 4,
      "username": "Gonna.eth",
      "admin": false,
      "moderator": true,
      "staff": true,
      "like_count": 1
    }
  ],
  "created_at": "2023-09-26T01:13:21.554Z"
}