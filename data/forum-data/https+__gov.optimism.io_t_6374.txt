{
  "title": "Building a Verifiable RetroPGF",
  "content": "Hey everyone,\n\nIsmael from Lagrange Labs here. I wanted to share a design that our team put\ntogether for a verifiable and trust-minimized RetroPGF reward distribution flow.\n\nWe’d love any feedback from the OP Collective and OP Delegates! Our team is\ncommitted to support with making this a reality, if the Collective feels that\nthis direction makes sense long-term.\n\n\nINTRODUCTION\n\nThe Optimism Collective’s Retroactive Public Goods Funding (RetroPGF) provides a\npowerful mechanism to incentivize the creation of public goods that support the\ndevelopment and usage of Optimism. With the third season of RetroPGF, the\nrewards are stratified across four categories: OP Stack, Collective Governance,\nDeveloper Ecosystem and End User Experience & Adoption.\n\nThe primary data for the categories Developer Ecosystem and Collective\nGovernance will likely be derived from objective assessments of on-chain\nactivity. We believe that there is an opportunity for the Optimism Collective to\nprogressively decentralize how these tokens are distributed for future seasons.\n\nIn the below post, we’ll outline a potential mechanism for a verifiable token\ndistribution based on two steps:\n\n 1. The OP Collective would commit to an anonymized reward function at the\n    beginning of a season.\n 2. At the end of a season, the OP collective would release funding recipients\n    and a proof of the reward function being correctly computed across all of\n    the Optimism blocks for that season.\n\nAlternatively, the OP collective could simply open source the function and a\ncircuit to generate a proof of eligibility, thereby amortizing the cost of\ncomputation across the funding recipients.\n\nTo handle generating proofs across this scale of data, we propose the use of\nLagrange Labs’ Zero-Knowledge MapReduce proof construction, which supports\nverifiable distributed computation at big data scale.\n\n\nUNDERSTANDING LAGRANGE’S ZERO-KNOWLEDGE MAPREDUCE:\n\nThe Lagrange Labs ZK Big Data stack is a proof construction optimized for\ngenerating large scale batch storage proofs concurrently with arbitrary dynamic\ndistributed computation. In short, a proof can be generated from a single block\nheader, proving both the data in an array of historical storage slots and the\nresults of a distributed computation executed on that data. Each proof combines\nboth the verification of storage inclusion and a proof of the result of a\nverifiable distributed computation, within a single step.\n\nAs opposed to a ZKVMs that proves the result of a sequential execution (e.g.\nLLVM, Risc5, WASM, etc), the ZK Big Data construction is optimized for proving\nlarge scale distributed computation (MapReduce, RDD/Spark, Distributed SQL, etc)\nin zero-knowledge.\n\nHow is this approach different from other storage proof solutions? The answer is\nscale.\n\nFor a RetroPGF season, a single proof of reward eligibility would have to show\non-chain activity across 3 months of Optimism blocks or ~19,440,000 blocks.\nConsider a standard depth of 8 summed across both the state trie and storage\ntrie. To access a single storage slot in a single contract over this period, a\nproof will have to process ~124.4 million Keccak hash functions and RLP\nencodings.\n\nIntuitively, as the sophistication of the rewards function increases and more\ndata is required per proof, the scale of computation quickly becomes\nunmanageable with current tooling.\n\n\nBUILDING A VERIFIABLE RETROPGF:\n\nA Predefined Rewards Function:\n\nTo decentralize the RetroPGF process, the OP Collective would first have to\ncommit to a rewards function at the start of a season. A simple commitment could\nbe made through deploying a verifier contract on Bedrock that is capable of\nreleasing OP tokens from a vault, based on a verification of a correct proof of\nthe reward function’s execution.\n\nIn order to minimize gaming of the rewards process, the function would be kept\nanonymous until the end of the season. With the Lagrange ZKMR stack, a rewards\nfunction would have access to any data available within historical Bedrock\nblockheaders. This data would include all contract storage (state trie),\ntransaction history (transaction trie) and event emissions (logs bloom & receipt\ntrie).\n\nAs mentioned previously, a computation run across data from 3 months of Optimism\nblocks (~19.5m blocks) would require significant computation resources. To\neffectively process this type of data at scale with the Lagrange ZKMR stack, the\nrewards function would be defined based on a parallelizable execution model.\nExamples of these execution models include MapReduce, Spark / RDD and\ndistributed SQL.\n\nConsider the example of a simple rewards function that distributes rewards to\nthe contracts that have the greatest number of users over a 3 month period.\nBelow is an example of how to specify the function in SQL, though it could also\neasily be written in either MapReduce or Spark.\n\nSELECT recipient,\n       COUNT (DISTINCT sender) AS \"uniqueusers\"\nFROM transactions\nWHERE blocknumber < x\n  AND blocknumber > x + 19440000 GROUP  BY recipient\n  ORDER  BY uniqueusers\nLIMIT 50;\n\n\n\nGENERATING A PROOF OF A DISTRIBUTED COMPUTATION:\n\nOnce a distributed computation is specified in a supported language, a proof of\nthe result of the computation can be generated through Lagrange’s ZKMR stack.\nThe ZKMR stack can be thought of as an extension of MapReduce that leverages\nrecursive proofs to prove the correctness of the distributed computation over\nlarge amounts of on-chain state data.\n\nThis is achieved by generating proofs of correctness for each given worker\nduring either the map or reduce steps of a distributed computation job. These\nproofs can be composed recursively to construct a single proof for the validity\nof the entire distributed workflow. In other words, the proofs of the smaller\ncomputations can be combined to create a proof of the entire computation.\n\nThe ability to compose multiple sub-proofs of worker computation into a single\nZKMR proof enables it to scale more efficiently for complex computations on\nlarge data sets, as would be required for a verifiable RetroPGF reward function.\n\nConsider the function above where rewards are distributed to the contracts that\nhave the greatest number of users over a 3 month period. In this example, the\nSELECT and WHERE operators would be translated into Map steps, while the LIMIT,\nGROUP BY, COUNT and ORDER BY operators would be translated into Reduce steps.\n\n\n\nimage\n[https://europe1.discourse-cdn.com/bc41dd/optimized/2X/4/4e2c0aa462a487c6e5970700ee0aa31c86ac3b2f_2_690x395.png]\nimage6285×3603 921 KB\n[https://europe1.discourse-cdn.com/bc41dd/original/2X/4/4e2c0aa462a487c6e5970700ee0aa31c86ac3b2f.png]\n\n\n\nWhile the above simple distribution strategy would likely be insufficient to\nproperly incentivize public goods creation, it nevertheless provides a starting\npoint of how RetroPGF rewards can be distributed in a verifiable manner. In\nreality, the Optimism Collective has full agency to develop and specify\nfunctions that correctly incentivize its users and developers based on on-chain\nactivity.\n\n\nNEXT STEPS\n\nWe believe that decentralization of the RetroPGF reward distribution process is\nan essential long-term step for ensuring that the OP Collective can continue its\nmission to build a sustainable future for Ethereum. Given the impact that public\ngood funding has on incentivizing protocol participation, ensuring that reward\ndistribution remains both honest and verifiable is an imperative.\n\nLagrange Labs will be soon announcing Version 1 of its proof construction, set\nto launch on testnet in the Summer of 2023. While the journey to full\ndecentralization on public goods funding may be a while away, we are committed\nto working with teams to support it becoming a reality.\n\nA big thank you to Mark Tyneway for his support in coming up with this design.\n\nOur main asks for the OP Collective and the OP Delegates are:\n\n 1. Would a trust minimized and verifiable RetroPGF be interesting to the OP\n    Collective long-term? If so, we’d love to work with everyone here to build\n    towards it together.\n\n 2. What are the distributed programming frameworks that members of the OP\n    Collective would most likely want to write computation in (MapReduce, SQL,\n    Spark, etc)?\n\n 3. What would be an interesting proof-of-concept for a verifiable RetroPGF\n    reward function?\n\n 4. Is access to data from other OP Stack based chains something that would be a\n    priority within a RetroPGF reward function (Base, Zora, etc)?",
  "views": 1209,
  "like_count": 4,
  "word_count": 1308,
  "answer": [],
  "created_at": "2023-07-03T23:10:36.028Z"
}