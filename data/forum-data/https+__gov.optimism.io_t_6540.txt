{
  "title": "LLMs assisting badgeholders",
  "content": "Hey everyone this is my first post on OP! I discussed an interesting idea with\n@Jonas [/u/jonas] after his EthCC talk, which he suggested I write up on this\nforum.\n\nThe Problem\n\nAn issue with RPGF is badgeholders getting overwhelmed with the large number of\nprojects they need to review. One of many adverse effects is that projects with\na known presence or ‘brand’ end up getting more votes - precisely what RetroPGF\nis supposed to guard against (reducing the role of marketing and letting impact\nspeak for itself). These issues are probably going to get worse with time as\nmore projects apply for funding through this mechanism.\n\nA Solution\n\nThe larger conceptual goal should involve moving the RPGF impact evaluation\nsystem from where we currently are on the right (a peer review system with\nexperts’ qualitative assessment) to the left (computational protocol with humans\nin the loop reviewing quantitative output).\n\n\n\nScreenshot 2023-07-30 at 5.33.16 PM\n[https://europe1.discourse-cdn.com/bc41dd/original/2X/7/70db52129ba66d0c11a364995eeab7384ba6c089.jpeg]\nScreenshot 2023-07-30 at 5.33.16 PM1084×650 63.5 KB\n[https://europe1.discourse-cdn.com/bc41dd/original/2X/7/70db52129ba66d0c11a364995eeab7384ba6c089.jpeg]\n\n\n\nThe idea is disbursing money based on composite scores from different LLMs\nreviewing & scoring project applications (a project with a score of 8/10 gets\ntwice the money as one with a 4/10).\n\nPilot\n\nAn easy way to get started would be through simulation. Feed project\napplications from RPGF round 2 to an LLM and ask it to give scores for each of\nthem. Map out how fund distribution via LLM scoring differs from the actual\ndistribution by badgeholders\n\nOver time, the role of badgeholders could evolve into evaluating the scores &\njustification provided by LLMs (‘Yes’ if the scoring is on point or ‘No’ if\nthere is a flaw in reasoning behind a score). If a score has a lot of ‘No’ votes\nfrom badgeholders, the flaws in reasoning are pointed out to the LLM so that it\nlearns from the feedback and re-tabulates all scores.\n\nWould be keen to hear from others in the community and whether this could be a\nworthwhile working group at the OP Collective !",
  "views": 1156,
  "like_count": 26,
  "word_count": 2602,
  "answer": [
    {
      "content": "Interesting idea. Would love to see the results for RPGF2 using this approach\nand compare the differences as mentioned in this post.\n\nWill these LLM weights be Open Source? If yes, with enough compute and\nreinforcement learning thrown at the problem, could submissions themselves use\nLLMs to game the output?",
      "created_at": "2023-07-31T10:31:26.570Z",
      "trust_level": 2,
      "username": "jengajojo",
      "admin": false,
      "moderator": false,
      "staff": false,
      "like_count": 4
    },
    {
      "content": "\n[https://dub1.discourse-cdn.com/bc41dd/user_avatar/gov.optimism.io/thedevanshmehta/48/5991_2.png]\nthedevanshmehta:\n\n> Although in my preliminary testing I did find that ‘EvaluatorGPT’ was\n> significantly better than Hugging Face.\n\nLooking at this, it seems like EvaluatorGPT is just running the GPT 4 in the\nbackend, so we go back to the same problem I mentioned in the last message of\nrelying on centralised entities. If we want open weights and open source like\n@jengajojo [/u/jengajojo] mentioned it’d have to be something like Erudito:\n\n[https://github.githubassets.com/favicons/favicon.svg] GitHub\n[https://github.com/adriacabeza/erudito]\n[https://europe1.discourse-cdn.com/bc41dd/optimized/2X/0/0692f34b1d2a614b7297e7bc124e8c84c43cefde_2_500x500.jpeg]\n\n\nGITHUB - ADRIACABEZA/ERUDITO: ERUDITO: EASY API/CLI TO ASK QUESTIONS ABOUT...\n[https://github.com/adriacabeza/erudito]\n\nErudito: Easy API/CLI to ask questions about your documentation - GitHub -\nadriacabeza/erudito: Erudito: Easy API/CLI to ask questions about your\ndocumentation\n\n\n\n\nAs I am not a citizen I haven’t even checked the context size of the usual RPGF\napplication, but it could be a fun experiment and an excuse to play with Llama 2\n:smile: [https://emoji.discourse-cdn.com/twitter/smile.png?v=12]\n\nAnother problem I thought about recently was that an LLM like this would\nprobably evaluate a good pitch as opposed to good impact. Other than nominations\nthemselves, what context would you give to the LLM for each potential recipient?",
      "created_at": "2023-08-02T16:58:12.127Z",
      "trust_level": 2,
      "username": "Oxytocin",
      "admin": false,
      "moderator": false,
      "staff": false,
      "like_count": 2
    },
    {
      "content": "Thanks @Jonas [/u/jonas] great to know that there’s interest to get this done!\n\nTo start with, i’m creating a project on buidlbox\n[https://app.buidlbox.io/ftc/fund-public-goods] to apply with this idea in the\nupcoming Funding the Commons hackathon. The gitcoin sponsored challenge on best\npublic goods funding project is a good fit\n\n\n\nScreenshot 2023-08-10 at 7.24.20 PM\n[https://europe1.discourse-cdn.com/bc41dd/optimized/2X/d/de88079978a358b76e3dae89f19364cdf578537d_2_504x500.jpeg]\nScreenshot 2023-08-10 at 7.24.20 PM1394×1382 102 KB\n[https://europe1.discourse-cdn.com/bc41dd/original/2X/d/de88079978a358b76e3dae89f19364cdf578537d.jpeg]\n\n\n\nHere’s the timeline and next steps\n\n 1. The round runs for a month with final submissions on 8th September '23, with\n    a $10k prize pool for compelling solutions.\n    We should have a good shot at this if we complete a MVP simulating the fund\n    distribution by badgeholders vs an LLM, with the stretch goal of letting\n    badgeholders correct output from the LLM if its reasoning is off kilter.\n\n 2. Team formation should complete by 16th August, when they are holding a\n    virtual mixer to find team mates.\n    Anyone interested in hacking this out should get in touch with me ASAP here\n    or on twitter/telegram @TheDevanshMehta [/u/thedevanshmehta]\n\n 3. We will require the dataset of all applications in RetroPGF round 2 which we\n    can feed to HuggingFace/GPT\n\n 4. We will need to speak with the RetroPGF team while constructing the prompts\n    to feed the LLM for evaluating/scoring projects (last week of August)\n\n 5. We do need some good team members that are familiar with the OP ecosystem\n    who can help in hacking this out over the next month",
      "created_at": "2023-08-10T14:13:02.563Z",
      "trust_level": 2,
      "username": "thedevanshmehta",
      "admin": false,
      "moderator": false,
      "staff": false,
      "like_count": 2
    },
    {
      "content": "In some updates on this thread, I have figured out how to use LLMs for\nquantifying outcomes in an impact report.\n\nHowever, doing it for organizations as a whole is far more challenging and would\nadvocate for some caution before jumping into it.\n\n 1. Open GPT assistant and upload the 1st chapter of relentless monetization\n    book (robin hood rules for smart giving) which lays out the method\n\n 2. After that, give the following instructions: This GPT is called Helen and\n    she is smart and asks questions if not sure of an answer. Helen is adept at\n    making projections and giving numbers that assign value to outcomes. If\n    Helen is not sure of what to take as a counterfactual or the right\n    assumptions in quantifying impact from a case study, she asks questions\n    until confident of the answer.\n\n 3. Copy paste the impact report submitted and then answer the questions Helen\n    asks until it provides an analysis or number\n\n 4. DO NOT make this calculation a single source of truth. To prevent bias or\n    arbitrariness, have at least 3 evaluators independently compute the benefit\n    cost ratio and then take the mean or median value. The more evaluators you\n    have, the more credible the quantification\n\n 5. Figure out whether you want funding tightly coupled with benefit cost ratio\n    analysis (flows mathematically to highest rated to lowest rated) or loosely\n    coupled (provides guidance to funders who still make the final allocation).\n\n 6. Ideally, redefine the relationship we have with projects from grantor :\n    grantee to customer : product. Each applicant submits their onchain impact\n    report, and when we fund them it results in a transfer of shards of their\n    impact report to us, making it an exchange rather than a grant\n\n 7. Determine the price of each impact report (and thus the % transferred to you\n    for funding provided) either on cost basis (how much it took to produce it)\n    or on benefit basis (the mean or median computation by all evaluators).",
      "created_at": "2024-02-18T07:22:10.435Z",
      "trust_level": 2,
      "username": "thedevanshmehta",
      "admin": false,
      "moderator": false,
      "staff": false,
      "like_count": 2
    },
    {
      "content": "I think it would be sick to run a small experiment on this! :man_scientist:\n[https://emoji.discourse-cdn.com/twitter/man_scientist.png?v=12]\nLLM’s could def help badgeholders make sense off all this information on impact\n& profit.",
      "created_at": "2023-08-08T10:50:54.702Z",
      "trust_level": 4,
      "username": "Jonas",
      "admin": true,
      "moderator": true,
      "staff": true,
      "like_count": 1
    }
  ],
  "created_at": "2023-07-30T16:03:17.120Z"
}